{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing on cuda\n",
      "Loading dataset from Torch tensor file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hizan\\AppData\\Local\\Temp\\ipykernel_20284\\3971068523.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(torch_dataset_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 18325, Val samples: 3926, Test samples: 3928\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Runing on {str(device)}\")\n",
    "\n",
    "# Define dataset paths\n",
    "data_dir = \"./animals10\"\n",
    "torch_dataset_path = \"./animals10Dataset.pt\"\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"cane\", \"cavallo\", \"elefante\", \"farfalla\", \"gallina\", \"gatto\", \"mucca\", \"pecora\", \"ragno\", \"scoiattolo\"]\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Load dataset from Torch tensor file\n",
    "print(\"Loading dataset from Torch tensor file...\")\n",
    "data = torch.load(torch_dataset_path)\n",
    "images, labels = data[\"images\"], data[\"labels\"]\n",
    "\n",
    "dataset = TensorDataset(images, labels)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.to(device)\n",
    "\n",
    "# Replace the final classification layer (model.fc) for 10 classes:\n",
    "num_ftrs = model.fc.in_features  # typically 2048 for ResNet50\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5. Define Loss and Optimizer\n",
    "# ------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust as needed\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"\n",
    "    Plots the training/validation loss and accuracy curves in separate charts.\n",
    "    \"\"\"\n",
    "    # Plot loss\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs   = []\n",
    "val_losses   = []\n",
    "val_accs     = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images_batch, labels_batch in train_loader:\n",
    "        images_batch = images_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images_batch)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track training loss\n",
    "        running_loss += loss.item() * images_batch.size(0)\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels_batch).sum().item()\n",
    "        total += labels_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accs.append(epoch_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images_batch, labels_batch in val_loader:\n",
    "            images_batch = images_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "\n",
    "            outputs = model(images_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            \n",
    "            # Track validation loss\n",
    "            val_running_loss += loss.item() * images_batch.size(0)\n",
    "            # Compute validation accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += predicted.eq(labels_batch).sum().item()\n",
    "            val_total += labels_batch.size(0)\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_epoch_acc = val_correct / val_total\n",
    "    \n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accs.append(val_epoch_acc)\n",
    "    \n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Accuracy\n",
    "plot_loss_and_accuracy(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_batch, labels_batch in test_loader:\n",
    "        images_batch = images_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        outputs = model(images_batch)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        \n",
    "        test_loss += loss.item() * images_batch.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_correct += predicted.eq(labels_batch).sum().item()\n",
    "        test_total += labels_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aps1052",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
